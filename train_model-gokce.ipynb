{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3419ebcd6673>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3419ebcd6673>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    def init_hidden(self)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.nn.init as torch_init\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\" A basic LSTM model. \n",
    "    \n",
    "    Consists of one hidden layer:\n",
    "    \n",
    "    conv1 -> conv2 -> conv3 -> maxpool -> fc1 -> fc2 (outputs)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, hid_dim, batch_size, no_layers =1):\n",
    "        super(LSTM, self).__init__()\n",
    "        #specify the input dimensions\n",
    "        self.in_dim = in_dim\n",
    "        #specify the output dimensions\n",
    "        self.out_dim = out_dim\n",
    "        #specify the batch size\n",
    "        self.batch_size = batch_size\n",
    "        #specify hidden layer dimensions\n",
    "        self.hid_dim = hid_dim\n",
    "        #specify the number of layers\n",
    "        self.no_layers = no_layers  \n",
    "        \n",
    "        #initialise the LSTM\n",
    "        self.model = nn.LSTM(self.in_dim, self.hid_dim, self.no_layers)\n",
    "        #define the outputs of the model, we're using a softmax\n",
    "        self.outputs = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Pass the batch of images through each layer of the network, applying \n",
    "        non-linearities after each layer.\n",
    "        \n",
    "        Note that this function *needs* to be called \"forward\" for PyTorch to \n",
    "        automagically perform the forward pass. \n",
    "        \n",
    "        Params:\n",
    "        -------\n",
    "        - batch: (Tensor) An input batch of images\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        - logits: (Variable) The output of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        #input \"batch\" is a tensor of dimensions batch_sizexchunk_sizexdictionary_size\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.model(batch.view(len(batch), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.outputs(lstm_out[-1].view(self.batch_size, -1))\n",
    "        tag_scores = func.softmax(y_pred, dim=1)\n",
    "        return tag_scores\n",
    "    def init_hidden(self)\n",
    "            return (torch.zeros(self.no_layers, self.batch_size, self.hid_dim),\n",
    "            torch.zeros(self.no_layers, self.batch_size, self.hid_dim))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "BasicCNN initialized\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "from baseline_cnn import *\n",
    "from baseline_cnn import BasicCNN\n",
    "import pdb\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 5           # Number of full passes through the dataset\n",
    "batch_size = 32          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "# Transform \n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "#baseline\n",
    "transform = transforms.Compose([transforms.Resize([512,512]),transforms.ToTensor()])\n",
    "#transform = transforms.Compose([transforms.Resize([512,512]),transforms.RandomRotation([-180,180]), \n",
    "#                                transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "                  \n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 2, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = BasicCNN()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "criterion =torch.nn.BCELoss() #TODO - loss criteria are defined in the torch.nn package\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = custom_bce()#didnt work like this\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #TODO - optimizers are defined in the torch.optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()  \n",
    "    for minibatch_count, (images, labels) in enumerate(val_loader, 0):\n",
    "        #print(minibatch_count)\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = custom_bce(outputs, labels)\n",
    "        \n",
    "        modelDecision=(outputs>=0.5)\n",
    "        accuracy = calculate_acc(modelDecision,labels)\n",
    "        losses.update(loss.item(), labels.shape[0])\n",
    "        acc.update(accuracy, labels.shape[0])\n",
    "\n",
    "    return acc,losses\n",
    "class AverageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def calculate_TP_FP_FN(modelDecision,labels):\n",
    "    modelDecNp=modelDecision.cpu().numpy()\n",
    "    labelsNp=labels.cpu().numpy()\n",
    "    numClasses=labelsNp.shape[1]\n",
    "    TP=np.zeros(numClasses)\n",
    "    FP=np.zeros(numClasses)\n",
    "    FN=np.zeros(numClasses)\n",
    "    for i in range(numClasses):\n",
    "        classOut=modelDecNp[:,i]\n",
    "        classLabel=labelsNp[:,i]\n",
    "        pos_label=np.argwhere(classLabel==1)\n",
    "        neg_label=np.argwhere(classLabel==0)\n",
    "        TP[i]=np.sum(classOut[pos_label])\n",
    "        FP[i]=np.sum(classOut[neg_label])\n",
    "        FN[i]=np.sum(classLabel)-TP[i]  \n",
    "    return TP,FP,FN\n",
    "\n",
    "def calculate_acc(modelDecision,labels):\n",
    "    acc=torch.sum((labels==modelDecision.to(dtype=torch.float)).to(dtype=torch.float),0)/labels.shape[0]\n",
    "    return acc\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 0 loss: 0.015, average accuracy: 0.525\n",
      "Epoch 1, average minibatch 50 loss: 0.717, average accuracy: 0.537\n",
      "Epoch 1, average minibatch 100 loss: 0.687, average accuracy: 0.577\n",
      "Epoch 1, average minibatch 150 loss: 0.660, average accuracy: 0.612\n",
      "Epoch 1, average minibatch 200 loss: 0.638, average accuracy: 0.639\n",
      "Epoch 1, average minibatch 250 loss: 0.614, average accuracy: 0.665\n",
      "Epoch 1, average minibatch 300 loss: 0.594, average accuracy: 0.688\n",
      "Epoch 1, average minibatch 350 loss: 0.574, average accuracy: 0.709\n",
      "Epoch 1, average minibatch 400 loss: 0.556, average accuracy: 0.729\n",
      "Epoch 1, average minibatch 450 loss: 0.540, average accuracy: 0.745\n",
      "Epoch 1, average minibatch 500 loss: 0.521, average accuracy: 0.761\n",
      "Epoch 1, average minibatch 550 loss: 0.505, average accuracy: 0.775\n",
      "Epoch 1, average minibatch 600 loss: 0.491, average accuracy: 0.788\n",
      "Epoch 1, average minibatch 650 loss: 0.475, average accuracy: 0.799\n",
      "Epoch 1, average minibatch 700 loss: 0.463, average accuracy: 0.808\n",
      "Epoch 1, average minibatch 750 loss: 0.450, average accuracy: 0.817\n",
      "Epoch 1, average minibatch 800 loss: 0.439, average accuracy: 0.825\n",
      "Epoch 1, average minibatch 850 loss: 0.427, average accuracy: 0.832\n",
      "Epoch 1, average minibatch 900 loss: 0.415, average accuracy: 0.838\n",
      "Epoch 1, average minibatch 950 loss: 0.408, average accuracy: 0.844\n",
      "Epoch 1, average minibatch 1000 loss: 0.396, average accuracy: 0.849\n",
      "Epoch 1, average minibatch 1050 loss: 0.385, average accuracy: 0.854\n",
      "Epoch 1, average minibatch 1100 loss: 0.378, average accuracy: 0.858\n",
      "Epoch 1, average minibatch 1150 loss: 0.368, average accuracy: 0.862\n",
      "Epoch 1, average minibatch 1200 loss: 0.361, average accuracy: 0.866\n",
      "Epoch 1, average minibatch 1250 loss: 0.353, average accuracy: 0.869\n",
      "Epoch 1, average minibatch 1300 loss: 0.347, average accuracy: 0.872\n",
      "Epoch 1, average minibatch 1350 loss: 0.339, average accuracy: 0.875\n",
      "Epoch 1, average minibatch 1400 loss: 0.330, average accuracy: 0.877\n",
      "Epoch 1, average minibatch 1450 loss: 0.325, average accuracy: 0.880\n",
      "Epoch 1, average minibatch 1500 loss: 0.318, average accuracy: 0.882\n",
      "Epoch 1, average minibatch 1550 loss: 0.315, average accuracy: 0.884\n",
      "Epoch 1, average minibatch 1600 loss: 0.309, average accuracy: 0.886\n",
      "Epoch 1, average minibatch 1650 loss: 0.304, average accuracy: 0.888\n",
      "Epoch 1, average minibatch 1700 loss: 0.297, average accuracy: 0.890\n",
      "Epoch 1, average minibatch 1750 loss: 0.288, average accuracy: 0.892\n",
      "Epoch 1, average minibatch 1800 loss: 0.286, average accuracy: 0.893\n",
      "Epoch 1, average minibatch 1850 loss: 0.283, average accuracy: 0.895\n",
      "Epoch 1, average minibatch 1900 loss: 0.278, average accuracy: 0.896\n",
      "Epoch 1, average minibatch 1950 loss: 0.277, average accuracy: 0.897\n",
      "Epoch 1, average minibatch 2000 loss: 0.272, average accuracy: 0.899\n",
      "Epoch 1, average minibatch 2050 loss: 0.268, average accuracy: 0.900\n",
      "Epoch 1, average minibatch 2100 loss: 0.266, average accuracy: 0.901\n",
      "Epoch 1, average minibatch 2150 loss: 0.265, average accuracy: 0.902\n",
      "Epoch 1, average minibatch 2200 loss: 0.260, average accuracy: 0.903\n",
      "Epoch 1, average minibatch 2250 loss: 0.256, average accuracy: 0.904\n",
      "Epoch 1, average minibatch 2300 loss: 0.253, average accuracy: 0.905\n",
      "Epoch 1, average minibatch 2350 loss: 0.246, average accuracy: 0.906\n",
      "Epoch 1, average minibatch 2400 loss: 0.245, average accuracy: 0.907\n",
      "Epoch 1, average minibatch 2450 loss: 0.240, average accuracy: 0.908\n",
      "Epoch 1, average minibatch 2500 loss: 0.238, average accuracy: 0.908\n",
      "Finished 1 epochs of training\n",
      "val_loss: 0.239, average accuracy of all classes: 0.947\n",
      "Epoch 2, average minibatch 0 loss: 0.004, average accuracy: 0.962\n",
      "Epoch 2, average minibatch 50 loss: 0.234, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 100 loss: 0.229, average accuracy: 0.950\n",
      "Epoch 2, average minibatch 150 loss: 0.232, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 200 loss: 0.230, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 250 loss: 0.226, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 300 loss: 0.222, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 350 loss: 0.222, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 400 loss: 0.222, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 450 loss: 0.219, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 500 loss: 0.215, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 550 loss: 0.210, average accuracy: 0.948\n",
      "Epoch 2, average minibatch 600 loss: 0.205, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 650 loss: 0.207, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 700 loss: 0.207, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 750 loss: 0.207, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 800 loss: 0.208, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 850 loss: 0.206, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 900 loss: 0.207, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 950 loss: 0.201, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1000 loss: 0.201, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1050 loss: 0.195, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1100 loss: 0.200, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1150 loss: 0.199, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1200 loss: 0.196, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1250 loss: 0.191, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1300 loss: 0.199, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1350 loss: 0.198, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1400 loss: 0.197, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1450 loss: 0.194, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1500 loss: 0.196, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1550 loss: 0.188, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1600 loss: 0.190, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1650 loss: 0.192, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1700 loss: 0.184, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1750 loss: 0.184, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1800 loss: 0.187, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1850 loss: 0.181, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1900 loss: 0.176, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 1950 loss: 0.189, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2000 loss: 0.181, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2050 loss: 0.177, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2100 loss: 0.183, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2150 loss: 0.183, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2200 loss: 0.189, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2250 loss: 0.181, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2300 loss: 0.176, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2350 loss: 0.181, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2400 loss: 0.189, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2450 loss: 0.189, average accuracy: 0.949\n",
      "Epoch 2, average minibatch 2500 loss: 0.179, average accuracy: 0.949\n",
      "Finished 2 epochs of training\n",
      "val_loss: 0.183, average accuracy of all classes: 0.948\n",
      "Epoch 3, average minibatch 0 loss: 0.003, average accuracy: 0.962\n",
      "Epoch 3, average minibatch 50 loss: 0.175, average accuracy: 0.950\n",
      "Epoch 3, average minibatch 100 loss: 0.176, average accuracy: 0.950\n",
      "Epoch 3, average minibatch 150 loss: 0.177, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 200 loss: 0.175, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 250 loss: 0.177, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 300 loss: 0.170, average accuracy: 0.950\n",
      "Epoch 3, average minibatch 350 loss: 0.175, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 400 loss: 0.174, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 450 loss: 0.185, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 500 loss: 0.179, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 550 loss: 0.173, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 600 loss: 0.171, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 650 loss: 0.174, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 700 loss: 0.172, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 750 loss: 0.171, average accuracy: 0.949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, average minibatch 800 loss: 0.176, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 850 loss: 0.175, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 900 loss: 0.173, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 950 loss: 0.171, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1000 loss: 0.177, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1050 loss: 0.172, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1100 loss: 0.170, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1150 loss: 0.182, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1200 loss: 0.169, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1250 loss: 0.175, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1300 loss: 0.172, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1350 loss: 0.173, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1400 loss: 0.165, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1450 loss: 0.166, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 1500 loss: 0.169, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1550 loss: 0.170, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1600 loss: 0.166, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1650 loss: 0.170, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1700 loss: 0.171, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1750 loss: 0.178, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1800 loss: 0.167, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1850 loss: 0.165, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1900 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 1950 loss: 0.172, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2000 loss: 0.167, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2050 loss: 0.169, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2100 loss: 0.167, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2150 loss: 0.171, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2200 loss: 0.173, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2250 loss: 0.170, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 2300 loss: 0.168, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 2350 loss: 0.173, average accuracy: 0.948\n",
      "Epoch 3, average minibatch 2400 loss: 0.160, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2450 loss: 0.173, average accuracy: 0.949\n",
      "Epoch 3, average minibatch 2500 loss: 0.170, average accuracy: 0.949\n",
      "Finished 3 epochs of training\n",
      "val_loss: 0.172, average accuracy of all classes: 0.948\n",
      "Epoch 4, average minibatch 0 loss: 0.004, average accuracy: 0.924\n",
      "Epoch 4, average minibatch 50 loss: 0.168, average accuracy: 0.946\n",
      "Epoch 4, average minibatch 100 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 150 loss: 0.167, average accuracy: 0.948\n",
      "Epoch 4, average minibatch 200 loss: 0.168, average accuracy: 0.948\n",
      "Epoch 4, average minibatch 250 loss: 0.158, average accuracy: 0.948\n",
      "Epoch 4, average minibatch 300 loss: 0.157, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 350 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 400 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 450 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 500 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 550 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 600 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 650 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 700 loss: 0.161, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 750 loss: 0.165, average accuracy: 0.948\n",
      "Epoch 4, average minibatch 800 loss: 0.155, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 850 loss: 0.165, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 900 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 950 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1000 loss: 0.158, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1050 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1100 loss: 0.166, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1150 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1200 loss: 0.167, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1250 loss: 0.156, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1300 loss: 0.160, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1350 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1400 loss: 0.157, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1450 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1500 loss: 0.165, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1550 loss: 0.160, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1600 loss: 0.158, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1650 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1700 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1750 loss: 0.161, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1800 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1850 loss: 0.160, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1900 loss: 0.167, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 1950 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2000 loss: 0.153, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2050 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2100 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2150 loss: 0.155, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2200 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2250 loss: 0.164, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2300 loss: 0.156, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2350 loss: 0.159, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2400 loss: 0.162, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2450 loss: 0.163, average accuracy: 0.949\n",
      "Epoch 4, average minibatch 2500 loss: 0.163, average accuracy: 0.949\n",
      "Finished 4 epochs of training\n",
      "val_loss: 0.169, average accuracy of all classes: 0.948\n",
      "Epoch 5, average minibatch 0 loss: 0.003, average accuracy: 0.953\n",
      "Epoch 5, average minibatch 50 loss: 0.145, average accuracy: 0.950\n",
      "Epoch 5, average minibatch 100 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 150 loss: 0.137, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 200 loss: 0.141, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 250 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 300 loss: 0.141, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 350 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 400 loss: 0.146, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 450 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 500 loss: 0.137, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 550 loss: 0.144, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 600 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 650 loss: 0.150, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 700 loss: 0.141, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 750 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 800 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 850 loss: 0.149, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 900 loss: 0.137, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 950 loss: 0.150, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1000 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1050 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1100 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1150 loss: 0.141, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1200 loss: 0.136, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1250 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1300 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1350 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1400 loss: 0.134, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1450 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1500 loss: 0.143, average accuracy: 0.951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, average minibatch 1550 loss: 0.140, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1600 loss: 0.137, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1650 loss: 0.135, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1700 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1750 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1800 loss: 0.144, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1850 loss: 0.136, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1900 loss: 0.140, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 1950 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2000 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2050 loss: 0.147, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2100 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2150 loss: 0.141, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2200 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2250 loss: 0.135, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2300 loss: 0.139, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2350 loss: 0.148, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2400 loss: 0.143, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2450 loss: 0.142, average accuracy: 0.951\n",
      "Epoch 5, average minibatch 2500 loss: 0.136, average accuracy: 0.951\n",
      "Finished 5 epochs of training\n",
      "val_loss: 0.181, average accuracy of all classes: 0.945\n",
      "Training complete after 4 epochs\n"
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "best_loss = 100\n",
    "acc_train_list=[]\n",
    "acc_val_list=[]\n",
    "\n",
    "loss_train_list=[]\n",
    "loss_val_list=[]\n",
    "\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    model.train(True)\n",
    "    N = 50\n",
    "    N_minibatch_loss = 0.0    \n",
    "    train_acc = AverageMeter()\n",
    "    \n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = custom_bce(outputs, labels)\n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #Calculate accuracy\n",
    "        modelDecision=(outputs>=0.5)\n",
    "        accuracy = calculate_acc(modelDecision,labels)\n",
    "        train_acc.update(accuracy, labels.shape[0])\n",
    "        \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        #\n",
    "        if minibatch_count % N == 0:    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f, average accuracy: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss,train_acc.avg.mean()))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "\n",
    "            \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    #TODO: Implement cross-validation\n",
    "    acc_val,loss_val = validate(val_loader, model, criterion)\n",
    "    \n",
    "    acc_train_list.append(train_acc.avg.mean())\n",
    "    acc_val_list.append((acc_val.avg).mean())\n",
    "    \n",
    "    loss_train_list.append(total_loss)\n",
    "    loss_val_list.append(loss_val.avg)\n",
    "    print('val_loss: %.3f, average accuracy of all classes: %.3f'%(loss_val.avg,(acc_val.avg).mean()))\n",
    "    \n",
    "    # remember best loss and save checkpoint\n",
    "    is_best = loss_val.avg <= best_loss\n",
    "    best_loss = max(loss_val.avg, best_loss)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'train_loader':train_loader,\n",
    "        'val_loader':val_loader,\n",
    "        'test_loader':test_loader,\n",
    "    }, is_best)\n",
    "    \n",
    "print(\"Training complete after\", epoch, \"epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = { \"acc_train_list\": acc_train_list, \"acc_val_list\": acc_val_list, \"loss_train_list\":loss_train_list,\"loss_val_list\":loss_val_list}\n",
    "pickle.dump( results, open( \"results_base.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pickle.load( open( \"results_base.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "modelDecision=(outputs>=0.5)\n",
    "#i)\n",
    "accuracy = calculate_acc(modelDecision,labels)\n",
    "#other evaluation on test\n",
    "TP,FP,FN = calculate_TP_FP_FN(modelDecision,labels)\n",
    "precision=TP/(FP+TP+np.finfo(float).eps)\n",
    "recall=TP/(TP+FN+np.finfo(float).eps)\n",
    "BCR=(precision+recall)/2.0\n",
    "agg_precision=np.mean(precision)\n",
    "agg_recall=np.mean(recall)\n",
    "agg_BCR=np.mean(BCR)\n",
    "\n",
    "def confusion_matrix(modelDecision,labels):\n",
    "    modelDecNp=modelDecision.cpu().numpy()\n",
    "    labelsNp=labels.cpu().numpy()\n",
    "    numClasses=labelsNp.shape[1]\n",
    "    #additional row and column for no disease\n",
    "    conf_mat=np.zeros((numClasses+1,numClasses+1))\n",
    "    ######\n",
    "    #There was also discussion in Piazza about how to do this. It is now clear with edge cases so I left this\n",
    "    #####\n",
    "    return conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
