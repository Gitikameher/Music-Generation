{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.nn.init as torch_init\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\" A basic LSTM model. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, hid_dim, batch_size, no_layers =1):\n",
    "        super(LSTM, self).__init__()\n",
    "        #specify the input dimensions\n",
    "        self.in_dim = in_dim\n",
    "        #specify the output dimensions\n",
    "        self.out_dim = out_dim\n",
    "        #specify hidden layer dimensions\n",
    "        self.hid_dim = hid_dim\n",
    "        #specify the number of layers\n",
    "        self.no_layers = no_layers  \n",
    "        #self.batch_size=batch_size\n",
    "        \n",
    "        #initialise the LSTM\n",
    "        self.model = nn.LSTM(self.in_dim, self.hid_dim, self.no_layers)\n",
    "        self.outputs = nn.Linear(self.hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, batch,hidden=None):\n",
    "        \"\"\"Pass the batch of images through each layer of the network, applying \n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.model(batch, hidden)\n",
    "        y_pred = self.outputs(lstm_out)\n",
    "        #The input is expected to contain raw, unnormalized scores for each class according to documentation\n",
    "        #tag_scores = func.softmax(y_pred,dim=2)\n",
    "        #return tag_scores,hidden\n",
    "        return y_pred,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "[27 81 82 ... 67 76 66]\n",
      "[81 82 63 ... 76 66 29]\n",
      "[27 81 82 ... 67 76 66]\n",
      "[81 82 63 ... 76 66 29]\n",
      "[27 81 82 ... 67 76 66]\n",
      "[81 82 63 ... 76 66 29]\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 5           # Number of full passes through the dataset\n",
    "batch_size = 16          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "#use_cuda=0\n",
    "use_cuda=torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size,shuffle=False, show_sample=False,extras=extras)\n",
    "\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = LSTM(in_dim=94, out_dim=94,hid_dim=100,batch_size=16,no_layers=1)\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() #TODO - loss criteria are defined in the torch.nn package\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 0 loss: 0.076\n",
      "Epoch 1, average minibatch 50 loss: 3.447\n",
      "Epoch 1, average minibatch 100 loss: 3.515\n",
      "Epoch 1, average minibatch 150 loss: 3.517\n",
      "Epoch 1, average minibatch 200 loss: 3.412\n",
      "Finished 1 epochs of training\n",
      "Epoch 2, average minibatch 0 loss: 0.074\n"
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "# In piazza @484 they said hidden state should be continues through batches.\n",
    "#initialise hidden layers\n",
    "\n",
    "hidden=None\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    N = 50\n",
    "    N_minibatch_loss = 0.0   \n",
    "    \n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "        if images.size()!=torch.Size([16, 100, 94]):\n",
    "            temp=16-images.size()[0]\n",
    "            a=np.array([93])\n",
    "            b=np.squeeze(np.eye(94)[a.reshape(-1)])\n",
    "            c=np.tile(b,(temp,100,1))\n",
    "            images=torch.cat((torch.tensor(c).float(),images.float()))\n",
    "            labels=torch.cat((torch.tensor(np.full((temp,100), [93])).float(),labels.float()))\n",
    "            labels=labels.long()\n",
    "        images=images.permute(1,0,2)\n",
    "        \n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        images=images.float()\n",
    "        labels=labels\n",
    "        \n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs,hidden = model(images,hidden)\n",
    "        outputs=outputs.permute(1,0,2)\n",
    "        \n",
    "#         outputs.shape => batchSize * sequenceSize *dictionarySize\n",
    "#         labels.shape => batchSize * sequenceSize\n",
    "        loss = criterion(outputs.contiguous().view(outputs.shape[0]*100, 94),labels.contiguous().view(-1))\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "        #TODO: Implement cross-validation\n",
    "        \n",
    "        if minibatch_count % N == 0:    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "        \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch, \"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
